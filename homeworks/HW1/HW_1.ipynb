{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW#1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTRE5lC73tET"
      },
      "source": [
        "# HW1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE-AP43U6ijK"
      },
      "source": [
        "### Assignment\n",
        "\n",
        "* Complete `LinearRegression` class such that it will be testable by next cells\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUzf1Mhe32OJ"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class LinearRegression:\n",
        "\n",
        "    def __init__(self, init_theta=None, alpha=0.01, n_iter=100):\n",
        "        '''\n",
        "        Constructor\n",
        "        '''\n",
        "        self.alpha = alpha\n",
        "        self.n_iter = n_iter\n",
        "        self.theta = init_theta\n",
        "        self.JHist = None\n",
        "\n",
        "    def gradientDescent(self, X, y, theta):\n",
        "        '''\n",
        "        Fits the model via gradient descent\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy matrix\n",
        "            y is an n-dimensional numpy vector\n",
        "            theta is a d-dimensional numpy vector\n",
        "        Returns:\n",
        "            the final theta found by gradient descent\n",
        "        '''\n",
        "        n,d = X.shape\n",
        "        self.JHist = []\n",
        "        for i in range(self.n_iter):\n",
        "            self.JHist.append((self.computeCost(X, y, theta), theta))\n",
        "            print(\"Iteration: \", i+1, \" Cost: \", self.JHist[i][0], \" Theta: \", theta)\n",
        "            # TODO:  add update equation here\n",
        "            \n",
        "        return theta\n",
        "\n",
        "    def computeCost(self, X, y, theta):\n",
        "        '''\n",
        "        Computes the objective function\n",
        "        Arguments:\n",
        "          X is a n-by-d numpy matrix\n",
        "          y is an n-dimensional numpy vector\n",
        "          theta is a d-dimensional numpy vector\n",
        "        Returns:\n",
        "          a scalar value of the cost  \n",
        "              ** make certain you don't return a matrix with just one value! **\n",
        "        '''\n",
        "        # TODO: add objective (cost) equation here\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        '''\n",
        "        Trains the model\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy matrix\n",
        "            y is an n-dimensional numpy vector\n",
        "        '''\n",
        "        n = len(y)\n",
        "        n,d = X.shape\n",
        "        if self.theta is None:\n",
        "            self.theta = np.matrix(np.zeros((d,1)))\n",
        "        self.theta = self.gradientDescent(X,y,self.theta)\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Used the model to predict values for each instance in X\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy matrix\n",
        "        Returns:\n",
        "            an n-dimensional numpy vector of the predictions\n",
        "        '''\n",
        "        # TODO:  add prediction function here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kR04d4K7OiH"
      },
      "source": [
        "* We hope you wrote bug-free code :) \n",
        "* Now it is time to test âœ…"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJRadPLmFi96"
      },
      "source": [
        "## Multivariate regression\n",
        "\n",
        "* The `data/multivariateData.dat` path must be accessable\n",
        "* Complete `TODO`s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AevUgIjb5Pjo"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import *\n",
        "\n",
        "# our linear regression class\n",
        "from linreg import LinearRegression\n",
        "\n",
        "# load the data\n",
        "filePath = \"data/multivariateData.dat\"\n",
        "file = open(filePath,'r')\n",
        "allData = np.loadtxt(file, delimiter=',')\n",
        "\n",
        "X = np.matrix(allData[:,:-1])\n",
        "y = np.matrix((allData[:,-1])).T\n",
        "\n",
        "n,d = X.shape\n",
        "\n",
        "# Standardize\n",
        "mean = X.mean(axis=0)\n",
        "std = X.std(axis=0)\n",
        "X = (X - mean) / std\n",
        "\n",
        "# Add a row of ones for the bias term\n",
        "X = np.c_[np.ones((n,1)), X]\n",
        "\n",
        "# initialize the model\n",
        "init_theta = np.matrix(np.random.randn((d+1))).T\n",
        "n_iter = 2000\n",
        "alpha = 0.01\n",
        "\n",
        "# Instantiate objects\n",
        "lr_model = LinearRegression(init_theta = init_theta, alpha = alpha, n_iter = n_iter)\n",
        "lr_model.fit(X,y)\n",
        "\n",
        "# Compute the closed form solution in one line of code\n",
        "thetaClosedForm = 0  # TODO:  replace \"0\" with closed form solution\n",
        "print(\"thetaClosedForm: \", thetaClosedForm)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxFMdT7WGpRg"
      },
      "source": [
        "## Univariate regression\n",
        "\n",
        "* The `data/univariateData.dat` path must be accessable\n",
        "* Complete `TODO`s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcVlc21I5cyw"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# Matplotlib provides matlab like plotting tools in python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# our linear regression class\n",
        "from linreg import LinearRegression\n",
        "\n",
        "# All the modules needed for 3d surface plots\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------------------------\n",
        "# Plotting tools already written for you.\n",
        "# Feel free edit and experiment.\n",
        "def plotData1DHelper(X, y):\n",
        "    plt.clf()\n",
        "    plt.title(\"Univariate Data\")\n",
        "    plt.xlabel(\"X\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.plot(X, y, 'rx', label='Training Data')\n",
        "\n",
        "\n",
        "def plotData1D(X, y):\n",
        "    '''\n",
        "        This function is to plot y vs X where the number of predictors of X is 1.\n",
        "        Input\n",
        "        X - n*1 matrix or vector of length n\n",
        "        y - n*1 matrix or vector of length n\n",
        "        to_block - boolean flag which when set stops the program execution until the \n",
        "            plot is closed\n",
        "    '''\n",
        "    plotData1DHelper(X, y)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plotRegLine1D( lr_model, X, y):\n",
        "    '''\n",
        "        Plots the y vs X and also the regressed line according to the theta computed.\n",
        "        Input\n",
        "        X - n*2 matrix or vector of length n ( the second dimension is a column of ones for the bias term)\n",
        "        y - n*1 matrix or vector of length n\n",
        "        lr_model - linear regression trained model\n",
        "    '''\n",
        "    plotData1DHelper(X[:,1], y)\n",
        "    plt.plot(X[:,1],X*lr_model.theta,'b-', label='Regression Line')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualizeObjective(lr_model,t1_vals,t2_vals, X, y):\n",
        "    '''\n",
        "        The function does the surface plot of the objective for a \n",
        "        univariate regression problem with a bias term, so over 2 parameters.\n",
        "        Search over the space of theta1, theta2.\n",
        "        \n",
        "        It also plots the gradient descent steps as blue points on the surface plot.\n",
        "        Finally it plots a contour plot of the same\n",
        "        \n",
        "        lr_model - object of class LinReg (already trained)\n",
        "        t1_vals, t2_vals - values over which the objective function should be plotted\n",
        "                        List of numbers\n",
        "        X - n*2 matrix or vector of length n ( the second dimension is a column of ones for the bias term)\n",
        "        y - n*1 matrix or vector of length n\n",
        "    '''\n",
        "    T1,T2 = np.meshgrid(t1_vals, t2_vals)\n",
        "    n,p = T1.shape\n",
        "\n",
        "    # Compute the objective function over the space\n",
        "    Z = np.zeros(T1.shape)\n",
        "    for i in range(n):\n",
        "        for j in range(p):\n",
        "            Z[i,j] = lr_model.computeCost(X,y, np.matrix([T1[i,j],T2[i,j]]).T )\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.gca(projection='3d')\n",
        "    surf = ax.plot_surface(T1, T2, Z, rstride=1, cstride=1, cmap=cm.coolwarm,\n",
        "        linewidth=0)\n",
        "\n",
        "    ax.zaxis.set_major_locator(LinearLocator(10))\n",
        "    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
        "\n",
        "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "\n",
        "    # If the history of the objective function plot the path taken by the gradient descent\n",
        "    if lr_model.JHist !=None:\n",
        "\n",
        "        for ii in range(len(lr_model.JHist)-1):\n",
        "            t1 = lr_model.JHist[ii][1].tolist()\n",
        "            t2 = lr_model.JHist[ii+1][1].tolist()\n",
        "\n",
        "            J1 = lr_model.JHist[ii][0]\n",
        "            J2 = lr_model.JHist[ii+1][0]\n",
        "            J1 = np.squeeze(np.array(J1))\n",
        "            J2 = np.squeeze(np.array(J2))\n",
        "\n",
        "            x_pts = [t1[0][0], t2[0][0]]\n",
        "            y_pts = [t1[1][0], t2[1][0]]\n",
        "            J_pts = [J1, J2]\n",
        "            ax.plot3D(x_pts, y_pts, J_pts, 'b-')\n",
        "\n",
        "        for J, t in lr_model.JHist:\n",
        "            J = [np.squeeze(np.array(J))]\n",
        "            t0 = [np.squeeze(np.array(t[0][0]))]\n",
        "            t1 = [np.squeeze(np.array(t[1][0]))]\n",
        "            ax.plot3D(t0, t1, J, 'mo')\n",
        "\n",
        "    plt.title('Surface plot of the cost function')\n",
        "    plt.xlabel('Theta0')\n",
        "    plt.ylabel('Theta1')\n",
        "    plt.show()\n",
        "\n",
        "    # Contour plot\n",
        "    plt.figure()\n",
        "    plt.clf()\n",
        "    CS = plt.contour(T1, T2, Z)\n",
        "    plt.clabel(CS, inline=1, fontsize=10)\n",
        "    plt.title('Contours of cost function')\n",
        "    plt.xlabel(\"Theta0\")\n",
        "    plt.ylabel(\"Theta1\")\n",
        "\n",
        "    plt.plot(lr_model.theta[0][0],lr_model.theta[1][0], 'rx')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# load the data\n",
        "filePath = \"data/univariateData.dat\"\n",
        "file = open(filePath,'r')\n",
        "allData = np.loadtxt(file, delimiter=',')\n",
        "\n",
        "X = np.matrix(allData[:,:-1])\n",
        "y = np.matrix((allData[:,-1])).T\n",
        "\n",
        "n,d = X.shape\n",
        "\n",
        "# Add a row of ones for the bias term\n",
        "X = np.c_[np.ones((n,1)), X]\n",
        "\n",
        "# initialize the model\n",
        "init_theta = np.matrix(np.ones((d+1,1)))*10  # note that we really should be initializing this to be near zero, but starting it near [10,10] works better to visualize gradient descent for this particular problem\n",
        "n_iter = 1500\n",
        "alpha = 0.01\n",
        "\n",
        "# Instantiate objects\n",
        "lr_model = LinearRegression(init_theta=init_theta, alpha=alpha, n_iter=n_iter)\n",
        "plotData1D(X[:,1],y)\n",
        "lr_model.fit(X,y)\n",
        "plotRegLine1D(lr_model, X, y)\n",
        "\n",
        "# Visualize the objective function convex shape\n",
        "theta1_vals = np.linspace(-10, 10, 100)\n",
        "theta2_vals = np.linspace(-10, 10, 100)\n",
        "visualizeObjective(lr_model,theta1_vals, theta2_vals, X, y)\n",
        "\n",
        "# Compute the closed form solution in one line of code\n",
        "theta_closed_form = 0  # TODO:  replace \"0\" with closed form solution\n",
        "print(\"theta_closed_form: \", theta_closed_form)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0KVoMcJG-6D"
      },
      "source": [
        "# Submission\n",
        "\n",
        "You must submit a (20,) array which is your prediction\n",
        "\n",
        "**Your grade might be calculated by this file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvQefda7HNY5"
      },
      "source": [
        "npzfile = np.load(\"test.npz\")\n",
        "\n",
        "array = npzfile['arr_0']\n",
        "\n",
        "# Predict the array\n",
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}